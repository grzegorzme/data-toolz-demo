{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moral-charity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from uuid import uuid4\n",
    "import pandas as pd\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minio\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"miniominio\"\n",
    "\n",
    "ENDPOINT_URL = \"http://0.0.0.0:9000\"\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-girlfriend",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-joining",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker-compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-addiction",
   "metadata": {},
   "source": [
    "* What is it\n",
    "  * why would I use it\n",
    "  * core building blocks\n",
    "  * use examples\n",
    "* instalation\n",
    "* core features\n",
    "  * FileSystem\n",
    "    * local\n",
    "    * s3\n",
    "  * DataIO\n",
    "    * parquet\n",
    "    * dsv\n",
    "      * types\n",
    "  * Bonus: JsonLogger\n",
    "  \n",
    "* notes\n",
    "  * no pre-checks - be careful what you try to read, write\n",
    "  * copy between bucket - possible but not always optimal\n",
    "  * test-covered but not perfect, found a bug during preparation\n",
    "  * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-capture",
   "metadata": {},
   "source": [
    "This notebook demonstrats the capabilities a `data-toolz` package. An open-source python package for handling filesystem I/O and conveniant Pandas wrapper features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-cleaners",
   "metadata": {},
   "source": [
    "# What is [data-toolz](https://pypi.org/project/data-toolz/)?\n",
    "`data-toolz` is an open-source python package providing convenient access to I/O operations for both local filesystem and cloud storage (currently AWS S3 supported), as well a layer for accessing data-like objects (`parquet`, `dsv`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-maple",
   "metadata": {},
   "source": [
    "## Why use it\n",
    "The thought behind creating this package was to standardize common and recuring I/O operations and minimize boilerplate code.\n",
    "\n",
    "Most \"data processes\" involve:\n",
    "* reading input\n",
    "* processing\n",
    "* writing output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-ghost",
   "metadata": {},
   "source": [
    "Let's look at a simple example of reading a file, processing it and storing the results locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-leave",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading/writing local files, using standard library\n",
    "\n",
    "with open(\"example-bucket/data.txt\") as file:\n",
    "    data = [int(item) for item in file]\n",
    "\n",
    "processed = list((item, f\"hello {item} from local\") for item in data)\n",
    "\n",
    "with open(\"example-bucket/processed-local.txt\", mode=\"wt\") as file:\n",
    "    for item in processed:\n",
    "        size = file.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-skating",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat example-bucket/processed-local.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-nutrition",
   "metadata": {},
   "source": [
    "And now the same operation in a cloud environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-mexican",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading/writing s3 files, using boto3\n",
    "\n",
    "import boto3\n",
    "\n",
    "s3_client = boto3.client(\"s3\", endpoint_url=ENDPOINT_URL)\n",
    "\n",
    "obj = s3_client.get_object(Bucket=\"example-bucket\", Key=\"data.txt\")\n",
    "data = [int(item) for item in obj[\"Body\"].read().decode(\"utf-8\").split()]\n",
    "\n",
    "processed = list((item, f\"hello {item} from s3\") for item in data)\n",
    "\n",
    "body = \"\".join(f\"{item}\\n\" for item in processed).encode(\"utf-8\")\n",
    "response = s3_client.put_object(Bucket=\"example-bucket\", Key=\"processed-s3.txt\", Body=body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-amendment",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat example-bucket/processed-s3.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-portable",
   "metadata": {},
   "source": [
    "And now the same steps using `data-toolz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-friendship",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datatoolz.filesystem import FileSystem\n",
    "\n",
    "fs = FileSystem()  # equivalent to `FileSystem(\"local\")`\n",
    "\n",
    "with fs.open(\"example-bucket/data.txt\") as file:\n",
    "    data = [int(item) for item in file]\n",
    "\n",
    "processed = list((item, f\"hello {item} from local datatoolz\") for item in data)\n",
    "\n",
    "with fs.open(\"example-bucket/processed-local-dt.txt\", mode=\"wt\") as file:\n",
    "    for item in processed:\n",
    "        size = file.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-canon",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat example-bucket/processed-local-dt.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-canada",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = FileSystem(\"s3\", endpoint_url=ENDPOINT_URL)\n",
    "\n",
    "with fs.open(\"example-bucket/data.txt\") as file:\n",
    "    data = [int(item) for item in file]\n",
    "\n",
    "processed = list((item, f\"hello {item} from s3 datatoolz\") for item in data)\n",
    "\n",
    "with fs.open(\"example-bucket/processed-s3-dt.txt\", mode=\"wt\") as file:\n",
    "    for item in processed:\n",
    "        size = file.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat example-bucket/processed-s3-dt.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-scottish",
   "metadata": {},
   "source": [
    "`data-toolz` allows to abstract the read and write part and use the same code both for local development as well as cloud deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-winning",
   "metadata": {},
   "source": [
    "## Core building blocks\n",
    "\n",
    "Source code can be found on Github: https://github.com/grzegorzme/data-toolz.\n",
    "The package is a wrapper around [fsspec](https://filesystem-spec.readthedocs.io/en/latest/) and [s3fs](https://s3fs.readthedocs.io/en/latest/)\n",
    "\n",
    "* `datatoolz.filesystem.FileSystem` - main entrypoint for accessing file system layer based on `fsspec.AbstractFileSystem`\n",
    "\n",
    "* `datatoolz.io.DataIO` - class for handling I/O operations on datasets. Accessed via two main methods\n",
    "  * `read(path, ...)`\n",
    "\n",
    "  * `write(dataframe, path, ...)`\n",
    "  \n",
    "* `datatoolz.logging.JsonLogger` - utlity class used for structured logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-tolerance",
   "metadata": {},
   "source": [
    "# Installation\n",
    "\n",
    "`data-toolz` is indexed on [PyPI](https://pypi.org/project/data-toolz/) and latest version can be installed via `pip`\n",
    "\n",
    "```bash\n",
    "pip install data-toolz\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-status",
   "metadata": {},
   "source": [
    "# Feature overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-shaft",
   "metadata": {},
   "source": [
    "## FileSystem (`datatoolz.filesystem.FileSystem`)\n",
    "\n",
    "This main entrypoint for accessing file system layer based on `fsspec.AbstractFileSystem`. It can be used to perform common file system operations like:\n",
    "* opening/writing files\n",
    "* listing/deleting files/folders\n",
    "* and few more depending on the underlying implementation\n",
    "\n",
    "\n",
    "Initialisation:\n",
    "\n",
    "```python\n",
    "from datatoolz.filesystem import FileSystem\n",
    "\n",
    "fs = FileSystem()                                       # simple instance pointing to local file system\n",
    "fs = FileSystem(\"local\")                                # same as above\n",
    "fs = FileSystem(\"s3\")                                   # pointer to s3 service\n",
    "fs = FileSystem(\"s3\", assumed_role=\"arn:aws:iam::123456789012:role/my-role\")  # s3 with custom access role\n",
    "fs = FileSystem(\"s3\", endpoint_url=\"s3.amazonaws.com\")  # custom endpoint url passed to the service client\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-occasions",
   "metadata": {},
   "source": [
    "### Writing and reading a file\n",
    "* `open` - open file in text/binary read/write mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-shelf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datatoolz.filesystem import FileSystem\n",
    "\n",
    "fs_name = \"local\"\n",
    "\n",
    "fs = FileSystem(fs_name, endpoint_url=ENDPOINT_URL)\n",
    "\n",
    "with fs.open(\"example-bucket/example.txt\", mode=\"wt\") as file:\n",
    "    size = file.write(f\"Hello {fs.name}!\")\n",
    "\n",
    "with fs.open(\"example-bucket/example.txt\", mode=\"rt\") as file:\n",
    "    file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra-instrumentation",
   "metadata": {},
   "source": [
    "### Basic operations\n",
    "\n",
    "* `ls` - list contents of folder\n",
    "\n",
    "* `mkdir` - create folder\n",
    "\n",
    "* `rm` - remove file/folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indirect-implement",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datatoolz.filesystem import FileSystem\n",
    "\n",
    "fs_name = \"local\"\n",
    "fs = FileSystem(fs_name, endpoint_url=ENDPOINT_URL)\n",
    "\n",
    "PATH = f\"example-bucket/new-directory-{str(uuid4())[:4]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-maple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new directory\n",
    "fs.mkdir(PATH)\n",
    "f\"Contents of {PATH}: {[item['name'] for item in fs.ls(PATH)]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-expert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write some files\n",
    "for i in range(3):\n",
    "    with fs.open(f\"{PATH}/{i}.txt\", mode=\"wt\") as file:\n",
    "        size = file.write(f\"Hello {i}\")\n",
    "\n",
    "f\"After adding files: {[item['name'] for item in fs.ls(PATH)]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-degree",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove file\n",
    "fs.rm(f\"{PATH}/1.txt\")\n",
    "\n",
    "f\"After removing single file: {[item['name'] for item in fs.ls(PATH)]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-football",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove whole folder\n",
    "fs.rm(PATH, recursive=True)\n",
    "\n",
    "# check the folder was removed\n",
    "[item[\"name\"] for item in fs.ls(\"example-bucket\") if item[\"type\"] == \"directory\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-luxury",
   "metadata": {},
   "source": [
    "### Other operations\n",
    "\n",
    "* `exists` - check if object exists\n",
    "\n",
    "* `copy` / `cp` - copy object between two location in the file system\n",
    "\n",
    "* `move` / `mv` - move object between two location in the file system\n",
    "\n",
    "* `walk` - walk the directory tree (see standard library method `os.walk`)\n",
    "\n",
    "* `isdir` - check if path is a directory\n",
    "\n",
    "* `disk_usage` / `du` - get disk usage of a path\n",
    "\n",
    "\n",
    "As the `FileSystem` object inherits from it's base class there are many more methods available (some may be restricted to `local` or `s3` types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-brisbane",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_name = \"s3\"\n",
    "fs = FileSystem(fs_name, endpoint_url=ENDPOINT_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-significance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check two paths\n",
    "for file_name in [\"example-bucket/example.txt\", \"example-bucket/non-existent.txt\"]:\n",
    "    f\"{file_name} exists: {fs.exists(file_name)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-technical",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move file\n",
    "fs.mv(\"example-bucket/example.txt\", \"example-bucket/non-existent.txt\")\n",
    "\n",
    "for file_name in [\"example-bucket/example.txt\", \"example-bucket/non-existent.txt\"]:\n",
    "    f\"{file_name} exists: {fs.exists(file_name)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-trademark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy file\n",
    "fs.cp(\"example-bucket/non-existent.txt\", \"example-bucket/example.txt\")\n",
    "\n",
    "for file_name in [\"example-bucket/example.txt\", \"example-bucket/non-existent.txt\"]:\n",
    "    f\"{file_name} exists: {fs.exists(file_name)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-pharmacology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup\n",
    "fs.rm(\"example-bucket/non-existent.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check if path is a directory\n",
    "for path in [\"example-bucket/example.txt\", \"example-bucket\"]:\n",
    "    f\"{path} is a directory: {fs.isdir(path)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-lottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# walk path\n",
    "for r, d, f in fs.walk(\"example-bucket\"):\n",
    "    r\n",
    "    d\n",
    "    f\n",
    "    \"-----\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-teens",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check disk usage\n",
    "f\"File disk usage: {fs.du('example-bucket/data.txt')}\"\n",
    "f\"Folder disk usage: {fs.du('example-bucket')}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-freedom",
   "metadata": {},
   "source": [
    "### Task\n",
    "How to copy/move files between two `s3` buckets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-dream",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy file `./example-bucket/data.txt` to `s3://example-bucket/new.txt`\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-marine",
   "metadata": {},
   "source": [
    "### AWS role-based access\n",
    "\n",
    "`FileSystem` allows to access a `s3` bucket via a provided role. This is useful when your current role does not have direct access to a bucket, but instead are allowed to assume a role which does.\n",
    "\n",
    "```python\n",
    "fs = FileSystem(\n",
    "    \"s3\", \n",
    "    endpoint_url=ENDPOINT_URL,\n",
    "    assumed_role=\"arn:aws:iam::123456789012:role/my-role\"\n",
    ")\n",
    "```\n",
    "\n",
    "Alternatively if you need to jump through an \"assume chain\" it is also possible\n",
    "\n",
    "```python\n",
    "fs = FileSystem(\n",
    "    \"s3\", \n",
    "    endpoint_url=ENDPOINT_URL,\n",
    "    assumed_role=[\"arn:aws:iam::123456789012:role/role-1\", \"arn:aws:iam::123456789012:role/role-2\"]\n",
    ")\n",
    "```\n",
    "\n",
    "Note that for the assume chain to work each role is assumed in succession, therefore each role in the provided list needs to be permitted to be assumed by the previous one.\n",
    "\n",
    "The assumed credentials are automatically refreshed in case your application runs for longer then one hour (AWS default for assume role action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-continent",
   "metadata": {},
   "source": [
    "## DataIO (datatoolz.io.DataIO)\n",
    "Is a wrapper class for reading and writing data files into/from a `pandas.DataFrame`.\n",
    "\n",
    "It exposes two main methods\n",
    "  * `read(path, ...)`\n",
    "  \n",
    "  * `write(dataframe, path, ...)`\n",
    "\n",
    "Initialisation:\n",
    "```python\n",
    "from datatoolz.io import DataIO\n",
    "from datatoolz.filesystem import FileSystem\n",
    "\n",
    "dio = DataIO()                     # basic instance pointing to local file system\n",
    "\n",
    "\n",
    "fs = FileSystem(...)\n",
    "dio = DataIO(filesystem=fs)        # instance pointing to a predefined file system (local/s3)\n",
    "\n",
    "\n",
    "def my_partition_transformer(prefix, partitions, values, suffix):\n",
    "    return \"string\"\n",
    "dio = DataIO(\n",
    "    partition_transformer=my_partition_transformer\n",
    ")                                  # instance with a custom `partition_transformer` callable\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-covering",
   "metadata": {},
   "source": [
    "### Basic writing and reading parquet\n",
    "* `write(..., filetype=\"parquet\")` - specified via the `filetype` argument, and is the default value if `filetype` is omited\n",
    "\n",
    "* `read(..., filetype=\"parquet\")` - same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-operator",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"col1\": [1, 2, 1], \"col2\": [\"a\", \"b\", \"c\"]})\n",
    "df\n",
    "\n",
    "from datatoolz.io import DataIO\n",
    "dio = DataIO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-passing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data to a given path(!)\n",
    "dio.write(df, \"my-parquet-file\")\n",
    "\n",
    "# read data from a give path(!)\n",
    "df_read = dio.read(\"my-parquet-file\")\n",
    "df_read\n",
    "\n",
    "# note the created folder/prefix - default settings do not result in idempotent operations!\n",
    "!tree && rm -rf my-parquet-file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-father",
   "metadata": {},
   "source": [
    "### Advanced writing\n",
    "* `write(..., suffix=...)` - specify a custom output suffix\n",
    "\n",
    "* `write(..., partition_by=...)` - specifies output partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "severe-worship",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"col1\": [1, 2, 1], \"col2\": [\"a\", \"b\", \"c\"]})\n",
    "df\n",
    "\n",
    "from datatoolz.io import DataIO\n",
    "dio = DataIO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-lancaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with `suffix=\"\"` the dataframe will be written under `path` as a single file\n",
    "dio.write(df, path=\"my-parquet-file\", suffix=\"\")\n",
    "dio.read(\"my-parquet-file\")\n",
    "\n",
    "!tree && rm -rf my-parquet-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-trader",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with `suffix=\"string\"` the dataframe will be written under `path/suffix` as a single file\n",
    "dio.write(df, path=\"my-parquet-file\", suffix=\"my-file\")\n",
    "dio.read(\"my-parquet-file\")\n",
    "\n",
    "!tree && rm -rf my-parquet-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-diving",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with `suffix=[\"string1\", \"string2\", ...]` the dataframe will be written in `path`\n",
    "# under multiple files listed in `suffix` (uniform split)\n",
    "dio.write(df, path=\"my-parquet-file\", suffix=[\"my-file-1\", \"my-file-2\"])\n",
    "dio.read(\"my-parquet-file\")\n",
    "\n",
    "# dio.read(\"my-parquet-file/my-file-1\")\n",
    "\n",
    "!tree && rm -rf my-parquet-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-window",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with `partition_by=[\"field\"]` the output in `path` will be additionally split by partition value\n",
    "dio.write(df, path=\"my-parquet-file\", partition_by=[\"col1\"])\n",
    "dio.read(\"my-parquet-file\")\n",
    "\n",
    "dio.read(\"my-parquet-file/col1=1\")\n",
    "\n",
    "!tree && rm -rf my-parquet-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-sheep",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the output can be partitioned by multiple fields `partition_by=[\"field1\", \"field2\", ...]`\n",
    "dio.write(df, path=\"my-parquet-file\", partition_by=[\"col1\", \"col2\"])\n",
    "dio.read(\"my-parquet-file\")\n",
    "\n",
    "dio.read(\"my-parquet-file/col1=1\")\n",
    "\n",
    "!tree && rm -rf my-parquet-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-tomorrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can overwrite the default partition_transformer resulting in different output path building e.g.\n",
    "# * default: `my-parquet-file/col1=1/1649112618853172000-25105f49-24dd-443b-b6bf-a5ca8f7a18d9`\n",
    "# * custom: `my-parquet-file/1/fixed-name`\n",
    "\n",
    "def custom_partition_transformer(prefix, partitions, values, suffix):\n",
    "    partition_part = \"/\".join(map(str, values))\n",
    "    return f\"{prefix}/{partition_part}/fixed-name\"\n",
    "\n",
    "dio = DataIO(partition_transformer=custom_partition_transformer)\n",
    "\n",
    "dio.write(df, path=\"my-parquet-file\", partition_by=[\"col1\"])\n",
    "dio.read(\"my-parquet-file\")\n",
    "\n",
    "!tree && rm -rf my-parquet-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additionally the partition columns can be dropped from output to reduce redundancy\n",
    "# this process is NOT REVERSABLE by default!!!\n",
    "dio = DataIO()\n",
    "\n",
    "dio.write(df, path=\"my-parquet-file\", partition_by=[\"col1\"], drop_partitions=True)\n",
    "dio.read(\"my-parquet-file/col1=1\")\n",
    "\n",
    "!tree && rm -rf my-parquet-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-navigation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "treated-somalia",
   "metadata": {},
   "source": [
    "### Other file types\n",
    "\n",
    "Basides `parquet` the following types are handled\n",
    "* `write(..., filetype=\"jsonlines\")`\n",
    "* `write(..., filetype=\"dsv\")`\n",
    "* `write(..., filetype=\"dsv\", **pandas_kwargs)`\n",
    "\n",
    "Both types support compression via `gzip=True`.\n",
    "\n",
    "`pandas_kwargs` is passed to `pandas.DataFrame.to_csv` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-plasma",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"col1\": [1, 2, 1], \"col2\": [\"a\", \"b\", \"c\"]})\n",
    "df\n",
    "\n",
    "from datatoolz.io import DataIO\n",
    "dio = DataIO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-planning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write gzip-ed jsonlines\n",
    "dio.write(df, path=\"my-data.json.gz\", filetype=\"jsonlines\", gzip=True, suffix=\"\")\n",
    "dio.read(\"my-data.json.gz\", filetype=\"jsonlines\", gzip=True)\n",
    "\n",
    "!tree && gunzip my-data.json.gz && cat my-data.json && rm my-data.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recreational-perth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write tab-separated (default) file\n",
    "dio.write(df, path=\"my-data.txt\", filetype=\"dsv\", suffix=\"\")\n",
    "dio.read(\"my-data.txt\", filetype=\"dsv\")\n",
    "\n",
    "!tree && cat my-data.txt && rm my-data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-alexander",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write |-separated file without header\n",
    "dio.write(df, path=\"my-data.txt\", filetype=\"dsv\", sep=\"|\", header=None, suffix=\"\")\n",
    "dio.read(\"my-data.txt\", filetype=\"dsv\", sep=\"|\", header=None)\n",
    "\n",
    "!tree && cat my-data.txt && rm my-data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-chester",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-toolz-demo",
   "language": "python",
   "name": "data-toolz-demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
