{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "moral-charity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from uuid import uuid4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "ENDPOINT_URL = \"http://0.0.0.0:9000\"\n",
    "os.environ[\"ENDPOINT_URL\"] = ENDPOINT_URL\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minio\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"miniominio\"\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-bench",
   "metadata": {},
   "source": [
    "### Initial file structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "systematic-girlfriend",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── \u001b[01;34manother-bucket\u001b[00m\r\n",
      "├── data-toolz-demo.ipynb\r\n",
      "├── docker-compose.yaml\r\n",
      "└── \u001b[01;34mexample-bucket\u001b[00m\r\n",
      "    └── data.txt\r\n",
      "\r\n",
      "2 directories, 3 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-metro",
   "metadata": {},
   "source": [
    "#### Start up s3 mock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deluxe-brave",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Running 1/0\r\n",
      "\u001b[34m ⠿ Container learnathon-minio-1  Running                                   0.0s\r\n",
      "\u001b[0m\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!docker-compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-filing",
   "metadata": {},
   "source": [
    "---\n",
    "# DATA-TOOLZ TUTORIAL\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-addiction",
   "metadata": {},
   "source": [
    "This notebook demonstrats the capabilities a `data-toolz` package. An open-source python package for handling filesystem I/O and conveniant `pandas` wrapper features.\n",
    "\n",
    "## Table of Contents:\n",
    "\n",
    "* [What is data-toolz](#What-is-data-toolz?)\n",
    "  * [Why use it](#Why-use-it)\n",
    "  * [Building blocks](#Building-blocks)\n",
    "* [Instalation](#Installation)\n",
    "* [Feature overview](#Feature-overview)\n",
    "  * [FileSystem](#FileSystem-(datatoolz.filesystem.FileSystem))\n",
    "    * [Write and read](#Write-and-read)\n",
    "    * [Basic operations](#Basic-operations)\n",
    "    * [More examples](#More-examples)\n",
    "    * [Exercise](#Exercise)\n",
    "    * [AWS role-based access](#AWS-role-based-access)\n",
    "  * [DataIO](#DataIO-(datatoolz.io.DataIO))\n",
    "    * [Basic writing and reading parquet](#Basic-writing-and-reading-parquet)\n",
    "    * [Advanced writing](#Advanced-writing)\n",
    "    * [Other file types](#Other-file-types)\n",
    "  * [JsonLogger](#JsonLogger-(datatoolz.logging.JsonLogger))\n",
    "* [Notes](#Notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-cleaners",
   "metadata": {},
   "source": [
    "---\n",
    "# What is [data-toolz](https://pypi.org/project/data-toolz/)?\n",
    "\n",
    "`data-toolz` is an open-source python package providing convenient access to I/O operations for both local filesystem and cloud storage (currently AWS S3 supported), as well a layer for accessing data-like objects (`parquet`, `jsonlines`, `dsv`).\n",
    "\n",
    "## Why use it\n",
    "\n",
    "The rationale behind creating this package was to standardize common and recuring I/O operations and minimize boilerplate code.\n",
    "\n",
    "Most data processes involve the following steps:\n",
    "* reading input\n",
    "* processing\n",
    "* writing output\n",
    "\n",
    "`data-toolz` goal is simplify the \"read\" and \"write\" steps providing a common interface for various file systems or file-system-like services."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-ghost",
   "metadata": {},
   "source": [
    "---\n",
    "Let's look at a simple example of reading a file, processing it and storing the results locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "placed-leave",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'hello 1 from local')\r\n",
      "(2, 'hello 2 from local')\r\n",
      "(3, 'hello 3 from local')\r\n",
      "(4, 'hello 4 from local')\r\n",
      "(5, 'hello 5 from local')\r\n"
     ]
    }
   ],
   "source": [
    "with open(\"example-bucket/data.txt\") as file:\n",
    "    data = [int(item) for item in file]\n",
    "\n",
    "processed = list((item, f\"hello {item} from local\") for item in data)\n",
    "\n",
    "with open(\"example-bucket/processed-local.txt\", mode=\"wt\") as file:\n",
    "    for item in processed:\n",
    "        size = file.write(f\"{item}\\n\")\n",
    "        \n",
    "!cat example-bucket/processed-local.txt && rm example-bucket/processed-local.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-nutrition",
   "metadata": {},
   "source": [
    "And now the same operation in a cloud environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "defined-province",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'hello 1 from s3')\n",
      "(2, 'hello 2 from s3')\n",
      "(3, 'hello 3 from s3')\n",
      "(4, 'hello 4 from s3')\n",
      "(5, 'hello 5 from s3')\n",
      "2022-04-04 21:52:15         10 data.txt\n",
      "2022-04-05 21:32:06        115 processed-s3.txt\n",
      "delete: s3://example-bucket/processed-s3.txt\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3_client = boto3.client(\"s3\", endpoint_url=ENDPOINT_URL)\n",
    "\n",
    "obj = s3_client.get_object(Bucket=\"example-bucket\", Key=\"data.txt\")\n",
    "data = [int(item) for item in obj[\"Body\"].read().decode(\"utf-8\").split()]\n",
    "\n",
    "processed = list((item, f\"hello {item} from s3\") for item in data)\n",
    "\n",
    "body = \"\".join(f\"{item}\\n\" for item in processed).encode(\"utf-8\")\n",
    "response = s3_client.put_object(Bucket=\"example-bucket\", Key=\"processed-s3.txt\", Body=body)\n",
    "\n",
    "!aws s3 --endpoint-url=$ENDPOINT_URL cp s3://example-bucket/processed-s3.txt  - | head\n",
    "!aws s3 --endpoint-url=$ENDPOINT_URL ls s3://example-bucket\n",
    "!aws s3 --endpoint-url=$ENDPOINT_URL rm s3://example-bucket/processed-s3.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-injury",
   "metadata": {},
   "source": [
    "---\n",
    "As you can probably see the interfacing with both storages looks very different, even though the performed operations are very similar.\n",
    "\n",
    "There are few possibilities to address this issue\n",
    "\n",
    "1. Replicate (cloud) production environment\n",
    "  * unix vs windows\n",
    "  * networking and connection issues\n",
    "  * changing storage type\n",
    "  * reflects production environment\n",
    "\n",
    "\n",
    "2. Write own storage interface\n",
    "  * flexible but needs maintanace\n",
    "  * risk of code coupling\n",
    "\n",
    "\n",
    "3. Use external dependency\n",
    "  * less boilerplate\n",
    "  * typical risks of using external packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-portable",
   "metadata": {},
   "source": [
    "---\n",
    "Let's see how `data-toolz` can help with the above task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "coordinated-friendship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'hello 1 from local datatoolz')\r\n",
      "(2, 'hello 2 from local datatoolz')\r\n",
      "(3, 'hello 3 from local datatoolz')\r\n",
      "(4, 'hello 4 from local datatoolz')\r\n",
      "(5, 'hello 5 from local datatoolz')\r\n"
     ]
    }
   ],
   "source": [
    "from datatoolz.filesystem import FileSystem\n",
    "\n",
    "fs = FileSystem()  # equivalent to `FileSystem(\"local\")`\n",
    "\n",
    "with fs.open(\"example-bucket/data.txt\") as file:\n",
    "    data = [int(item) for item in file]\n",
    "\n",
    "processed = list((item, f\"hello {item} from local datatoolz\") for item in data)\n",
    "\n",
    "with fs.open(\"example-bucket/processed-local-dt.txt\", mode=\"wt\") as file:\n",
    "    for item in processed:\n",
    "        size = file.write(f\"{item}\\n\")\n",
    "\n",
    "!cat example-bucket/processed-local-dt.txt && rm example-bucket/processed-local-dt.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "familiar-canada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'hello 1 from s3 datatoolz')\n",
      "(2, 'hello 2 from s3 datatoolz')\n",
      "(3, 'hello 3 from s3 datatoolz')\n",
      "(4, 'hello 4 from s3 datatoolz')\n",
      "(5, 'hello 5 from s3 datatoolz')\n",
      "2022-04-04 21:52:15         10 data.txt\n",
      "2022-04-05 21:32:14        165 processed-s3-dt.txt\n",
      "delete: s3://example-bucket/processed-s3-dt.txt\n"
     ]
    }
   ],
   "source": [
    "from datatoolz.filesystem import FileSystem\n",
    "\n",
    "fs = FileSystem(\"s3\", endpoint_url=ENDPOINT_URL)\n",
    "\n",
    "with fs.open(\"example-bucket/data.txt\") as file:\n",
    "    data = [int(item) for item in file]\n",
    "\n",
    "processed = list((item, f\"hello {item} from s3 datatoolz\") for item in data)\n",
    "\n",
    "with fs.open(\"example-bucket/processed-s3-dt.txt\", mode=\"wt\") as file:\n",
    "    for item in processed:\n",
    "        size = file.write(f\"{item}\\n\")\n",
    "\n",
    "!aws s3 --endpoint-url=$ENDPOINT_URL cp s3://example-bucket/processed-s3-dt.txt  - | head\n",
    "!aws s3 --endpoint-url=$ENDPOINT_URL ls s3://example-bucket\n",
    "!aws s3 --endpoint-url=$ENDPOINT_URL rm s3://example-bucket/processed-s3-dt.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-scottish",
   "metadata": {},
   "source": [
    "`data-toolz` allows to abstract the read and write operations and use the same code both for local development as well as cloud deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-winning",
   "metadata": {},
   "source": [
    "---\n",
    "## Building blocks\n",
    "\n",
    "Source code can be found on Github: https://github.com/grzegorzme/data-toolz.\n",
    "The package is a wrapper around [fsspec](https://filesystem-spec.readthedocs.io/en/latest/) and [s3fs](https://s3fs.readthedocs.io/en/latest/)\n",
    "\n",
    "* `datatoolz.filesystem.FileSystem` - main entrypoint for accessing file system layer based on `fsspec.AbstractFileSystem`\n",
    "\n",
    "\n",
    "* `datatoolz.io.DataIO` - class for handling I/O operations on datasets. Accessed via two main methods\n",
    "\n",
    "  * `read(path, ...)`\n",
    "\n",
    "  * `write(dataframe, path, ...)`\n",
    "  \n",
    "\n",
    "* `datatoolz.logging.JsonLogger` - utlity class used for structured logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-tolerance",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Installation\n",
    "\n",
    "`data-toolz` is indexed on [PyPI](https://pypi.org/project/data-toolz/) and latest version can be installed via `pip`\n",
    "\n",
    "```bash\n",
    "pip install data-toolz\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-italic",
   "metadata": {},
   "source": [
    "---\n",
    "# Feature overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-shaft",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## FileSystem (`datatoolz.filesystem.FileSystem`)\n",
    "\n",
    "This main entrypoint for accessing file system layer based on `fsspec.AbstractFileSystem`. It can be used to perform common file system operations like:\n",
    "* opening/writing files\n",
    "* listing/deleting files/folders\n",
    "* and few more depending on the underlying implementation\n",
    "\n",
    "\n",
    "Initialisation:\n",
    "\n",
    "```python\n",
    "from datatoolz.filesystem import FileSystem\n",
    "\n",
    "fs = FileSystem()                                       # simple instance pointing to local file system\n",
    "fs = FileSystem(\"local\")                                # same as above\n",
    "fs = FileSystem(\"s3\")                                   # pointer to s3 service\n",
    "fs = FileSystem(\"s3\", assumed_role=\"arn:aws:iam::123456789012:role/my-role\")  # s3 with custom access role\n",
    "fs = FileSystem(\"s3\", endpoint_url=\"s3.amazonaws.com\")  # custom endpoint url passed to the service client\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-occasions",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Write and read\n",
    "* `open` - open file in text/binary read/write mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "international-limit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello local!\n",
      "\u001b[01;34m.\u001b[00m\n",
      "├── \u001b[01;34manother-bucket\u001b[00m\n",
      "├── data-toolz-demo.ipynb\n",
      "├── docker-compose.yaml\n",
      "└── \u001b[01;34mexample-bucket\u001b[00m\n",
      "    ├── data.txt\n",
      "    └── example.txt\n",
      "\n",
      "2 directories, 4 files\n"
     ]
    }
   ],
   "source": [
    "from datatoolz.filesystem import FileSystem\n",
    "\n",
    "fs_name = \"local\"\n",
    "\n",
    "fs = FileSystem(fs_name, endpoint_url=ENDPOINT_URL)\n",
    "\n",
    "with fs.open(\"example-bucket/example.txt\", mode=\"wt\") as file:\n",
    "    size = file.write(f\"Hello {fs.name}!\")\n",
    "\n",
    "with fs.open(\"example-bucket/example.txt\", mode=\"rt\") as file:\n",
    "    print(file.read())\n",
    "    \n",
    "!tree && rm example-bucket/example.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-reproduction",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 1\n",
    "How would to write and read a binary object (think `pickle`)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "regulation-shelf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra-instrumentation",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Basic operations\n",
    "\n",
    "* `ls` - list contents of folder\n",
    "\n",
    "* `mkdir` - create folder\n",
    "\n",
    "* `rm` - remove file/folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "indirect-implement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── \u001b[01;34manother-bucket\u001b[00m\r\n",
      "├── data-toolz-demo.ipynb\r\n",
      "├── docker-compose.yaml\r\n",
      "└── \u001b[01;34mexample-bucket\u001b[00m\r\n",
      "    └── data.txt\r\n",
      "\r\n",
      "2 directories, 3 files\r\n"
     ]
    }
   ],
   "source": [
    "from datatoolz.filesystem import FileSystem\n",
    "\n",
    "fs_name = \"s3\"\n",
    "fs = FileSystem(fs_name, endpoint_url=ENDPOINT_URL)\n",
    "\n",
    "PATH = f\"example-bucket/new-directory-{str(uuid4())[:4]}\"\n",
    "\n",
    "!tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-virginia",
   "metadata": {},
   "source": [
    "---\n",
    "#### Create a new directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cross-maple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── \u001b[01;34manother-bucket\u001b[00m\r\n",
      "├── data-toolz-demo.ipynb\r\n",
      "├── docker-compose.yaml\r\n",
      "└── \u001b[01;34mexample-bucket\u001b[00m\r\n",
      "    └── data.txt\r\n",
      "\r\n",
      "2 directories, 3 files\r\n"
     ]
    }
   ],
   "source": [
    "fs.mkdir(PATH)\n",
    "\n",
    "!tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-setting",
   "metadata": {},
   "source": [
    "---\n",
    "#### Write couple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "becoming-expert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── \u001b[01;34manother-bucket\u001b[00m\r\n",
      "├── data-toolz-demo.ipynb\r\n",
      "├── docker-compose.yaml\r\n",
      "└── \u001b[01;34mexample-bucket\u001b[00m\r\n",
      "    ├── data.txt\r\n",
      "    └── \u001b[01;34mnew-directory-5d2e\u001b[00m\r\n",
      "        ├── 0.txt\r\n",
      "        ├── 1.txt\r\n",
      "        └── 2.txt\r\n",
      "\r\n",
      "3 directories, 6 files\r\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    with fs.open(f\"{PATH}/{i}.txt\", mode=\"wt\") as file:\n",
    "        size = file.write(f\"Hello {i}\")\n",
    "\n",
    "!tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-spectrum",
   "metadata": {},
   "source": [
    "---\n",
    "#### List contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cubic-measurement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Key': 'example-bucket/data.txt',\n",
       "  'LastModified': datetime.datetime(2022, 4, 4, 19, 52, 15, 650000, tzinfo=tzutc()),\n",
       "  'ETag': '\"00000000000000000000000000000000-1\"',\n",
       "  'Size': 10,\n",
       "  'StorageClass': 'STANDARD',\n",
       "  'Owner': {'DisplayName': 'minio',\n",
       "   'ID': '02d6176db174dc93cb1b899f7c6078f08654445fe8cf1b6ce98d8855f66bdbf4'},\n",
       "  'type': 'file',\n",
       "  'size': 10,\n",
       "  'name': 'example-bucket/data.txt'},\n",
       " {'Key': 'example-bucket/new-directory-5d2e',\n",
       "  'Size': 0,\n",
       "  'StorageClass': 'DIRECTORY',\n",
       "  'type': 'directory',\n",
       "  'size': 0,\n",
       "  'name': 'example-bucket/new-directory-5d2e'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs.ls(\"example-bucket\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-corpus",
   "metadata": {},
   "source": [
    "---\n",
    "#### Remove file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "criminal-degree",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── \u001b[01;34manother-bucket\u001b[00m\r\n",
      "├── data-toolz-demo.ipynb\r\n",
      "├── docker-compose.yaml\r\n",
      "└── \u001b[01;34mexample-bucket\u001b[00m\r\n",
      "    ├── data.txt\r\n",
      "    └── \u001b[01;34mnew-directory-5d2e\u001b[00m\r\n",
      "        ├── 0.txt\r\n",
      "        └── 2.txt\r\n",
      "\r\n",
      "3 directories, 5 files\r\n"
     ]
    }
   ],
   "source": [
    "fs.rm(f\"{PATH}/1.txt\")\n",
    "\n",
    "!tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-country",
   "metadata": {},
   "source": [
    "---\n",
    "#### Remove whole folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "unknown-football",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── \u001b[01;34manother-bucket\u001b[00m\r\n",
      "├── data-toolz-demo.ipynb\r\n",
      "├── docker-compose.yaml\r\n",
      "└── \u001b[01;34mexample-bucket\u001b[00m\r\n",
      "    └── data.txt\r\n",
      "\r\n",
      "2 directories, 3 files\r\n"
     ]
    }
   ],
   "source": [
    "fs.rm(PATH, recursive=True)\n",
    "\n",
    "!tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-luxury",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### More examples\n",
    "\n",
    "* `exists` - check if object exists\n",
    "\n",
    "* `copy` / `cp` - copy object between two location in the file system\n",
    "\n",
    "* `move` / `mv` - move object between two location in the file system\n",
    "\n",
    "* `walk` - walk the directory tree (see standard library method `os.walk`)\n",
    "\n",
    "* `isdir` - check if path is a directory\n",
    "\n",
    "* `disk_usage` / `du` - get disk usage of a path\n",
    "\n",
    "\n",
    "As the `FileSystem` object inherits from it's base class there are many more methods available (some may be restricted to `local` or `s3` types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "local-brisbane",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_name = \"s3\"\n",
    "fs = FileSystem(fs_name, endpoint_url=ENDPOINT_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-teens",
   "metadata": {},
   "source": [
    "---\n",
    "#### Check if file exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "every-significance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example-bucket/data.txt exists: True\n",
      "example-bucket/non-existent.txt exists: False\n",
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── \u001b[01;34manother-bucket\u001b[00m\r\n",
      "├── data-toolz-demo.ipynb\r\n",
      "├── docker-compose.yaml\r\n",
      "└── \u001b[01;34mexample-bucket\u001b[00m\r\n",
      "    └── data.txt\r\n",
      "\r\n",
      "2 directories, 3 files\r\n"
     ]
    }
   ],
   "source": [
    "for file_name in [\"example-bucket/data.txt\", \"example-bucket/non-existent.txt\"]:\n",
    "    print(f\"{file_name} exists: {fs.exists(file_name)}\")\n",
    "    \n",
    "!tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-cincinnati",
   "metadata": {},
   "source": [
    "---\n",
    "#### Copy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "phantom-technical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── \u001b[01;34manother-bucket\u001b[00m\r\n",
      "├── data-toolz-demo.ipynb\r\n",
      "├── docker-compose.yaml\r\n",
      "└── \u001b[01;34mexample-bucket\u001b[00m\r\n",
      "    ├── data.txt\r\n",
      "    └── non-existent.txt\r\n",
      "\r\n",
      "2 directories, 4 files\r\n"
     ]
    }
   ],
   "source": [
    "fs.cp(\"example-bucket/data.txt\", \"example-bucket/non-existent.txt\")\n",
    "\n",
    "!tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-connecticut",
   "metadata": {},
   "source": [
    "---\n",
    "#### Move file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "automated-trademark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── \u001b[01;34manother-bucket\u001b[00m\r\n",
      "├── data-toolz-demo.ipynb\r\n",
      "├── docker-compose.yaml\r\n",
      "└── \u001b[01;34mexample-bucket\u001b[00m\r\n",
      "    ├── data.txt\r\n",
      "    └── new.txt\r\n",
      "\r\n",
      "2 directories, 4 files\r\n"
     ]
    }
   ],
   "source": [
    "fs.mv(\"example-bucket/non-existent.txt\", \"example-bucket/new.txt\")\n",
    "\n",
    "!tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-found",
   "metadata": {},
   "source": [
    "---\n",
    "#### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "statewide-pharmacology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── \u001b[01;34manother-bucket\u001b[00m\r\n",
      "├── data-toolz-demo.ipynb\r\n",
      "├── docker-compose.yaml\r\n",
      "└── \u001b[01;34mexample-bucket\u001b[00m\r\n",
      "    └── data.txt\r\n",
      "\r\n",
      "2 directories, 3 files\r\n"
     ]
    }
   ],
   "source": [
    "fs.rm(\"example-bucket/new.txt\")\n",
    "\n",
    "!tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-class",
   "metadata": {},
   "source": [
    "---\n",
    "#### Check if path is a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "compatible-minneapolis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example-bucket/example.txt is a directory: False\n",
      "example-bucket is a directory: True\n",
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── \u001b[01;34manother-bucket\u001b[00m\r\n",
      "├── data-toolz-demo.ipynb\r\n",
      "├── docker-compose.yaml\r\n",
      "└── \u001b[01;34mexample-bucket\u001b[00m\r\n",
      "    └── data.txt\r\n",
      "\r\n",
      "2 directories, 3 files\r\n"
     ]
    }
   ],
   "source": [
    "for path in [\"example-bucket/example.txt\", \"example-bucket\"]:\n",
    "    print(f\"{path} is a directory: {fs.isdir(path)}\")\n",
    "\n",
    "!tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-floor",
   "metadata": {},
   "source": [
    "---\n",
    "#### Walk path structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dominant-lottery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root = example-bucket\n",
      "dirs = []\n",
      "files = ['data.txt']\n",
      "\n",
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── \u001b[01;34manother-bucket\u001b[00m\r\n",
      "├── data-toolz-demo.ipynb\r\n",
      "├── docker-compose.yaml\r\n",
      "└── \u001b[01;34mexample-bucket\u001b[00m\r\n",
      "    └── data.txt\r\n",
      "\r\n",
      "2 directories, 3 files\r\n"
     ]
    }
   ],
   "source": [
    "for r, d, f in fs.walk(\"example-bucket\"):\n",
    "    print(f\"root = {r}\\ndirs = {d}\\nfiles = {f}\\n\")\n",
    "    \n",
    "!tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-corporation",
   "metadata": {},
   "source": [
    "---\n",
    "#### Check disk usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ranging-teens",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File disk usage: 10\n",
      "Folder disk usage: 10\n",
      "\n",
      "4.0K\texample-bucket/data.txt\r\n"
     ]
    }
   ],
   "source": [
    "print(f\"File disk usage: {fs.du('example-bucket/data.txt')}\")\n",
    "print(f\"Folder disk usage: {fs.du('example-bucket')}\\n\")\n",
    "\n",
    "!du -h example-bucket/data.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-freedom",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 2\n",
    "1. How to copy/move files between two `s3` buckets?\n",
    "2. What if buckets your bucket access is via different roles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "impressed-dream",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy file `s3://example-bucket/data.txt` to `s3://another-bucket/new-data.txt`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-mirror",
   "metadata": {},
   "source": [
    "---\n",
    "### AWS role-based access\n",
    "\n",
    "`FileSystem` allows to access a `s3` bucket via a provided role. This is useful when your current role does not have direct access to a bucket, but instead are allowed to assume a role which does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "coated-sending",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if run on mocked setup this will fail as access to `sts` service is required\n",
    "# fs = FileSystem(\n",
    "#     \"s3\", \n",
    "#     endpoint_url=ENDPOINT_URL,\n",
    "#     assumed_role=\"arn:aws:iam::123456789012:role/my-role\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-exception",
   "metadata": {},
   "source": [
    "Alternatively if you need to jump through an \"assume chain\" it is also possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "tested-cartridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if run on mocked setup this will fail as access to `sts` service is required\n",
    "# fs = FileSystem(\n",
    "#     \"s3\", \n",
    "#     endpoint_url=ENDPOINT_URL,\n",
    "#     assumed_role=[\"arn:aws:iam::123456789012:role/role-1\", \"arn:aws:iam::123456789012:role/role-2\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-marine",
   "metadata": {},
   "source": [
    "Note that for the assume chain to work each role is assumed in succession, therefore each role in the provided list needs to be permitted to be assumed by the previous one.\n",
    "\n",
    "The assumed credentials are automatically refreshed in case your application runs for longer then one hour (AWS default for assume role action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-continent",
   "metadata": {},
   "source": [
    "## DataIO (datatoolz.io.DataIO)\n",
    "Is a wrapper class for reading and writing data files into/from a `pandas.DataFrame`.\n",
    "\n",
    "It exposes two main methods\n",
    "  * `read(path, ...)`\n",
    "  \n",
    "  * `write(dataframe, path, ...)`\n",
    "\n",
    "Initialisation:\n",
    "```python\n",
    "from datatoolz.io import DataIO\n",
    "from datatoolz.filesystem import FileSystem\n",
    "\n",
    "dio = DataIO()                     # basic instance pointing to local file system\n",
    "\n",
    "\n",
    "fs = FileSystem(...)\n",
    "dio = DataIO(filesystem=fs)        # instance pointing to a predefined file system (local/s3)\n",
    "\n",
    "\n",
    "def my_partition_transformer(prefix, partitions, values, suffix):\n",
    "    return \"string\"\n",
    "dio = DataIO(\n",
    "    partition_transformer=my_partition_transformer\n",
    ")                                  # instance with a custom `partition_transformer` callable\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-covering",
   "metadata": {},
   "source": [
    "---\n",
    "### Basic writing and reading parquet\n",
    "\n",
    "* `write(..., filetype=\"parquet\")` - specified via the `filetype` argument, and is the default value if `filetype` is omited\n",
    "\n",
    "* `read(..., filetype=\"parquet\")` - same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "demographic-operator",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"col1\": [1, 2, 1], \"col2\": [\"a\", \"b\", \"c\"]})\n",
    "\n",
    "from datatoolz.io import DataIO\n",
    "dio = DataIO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "graphic-passing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col1 col2\n",
       "0     1    a\n",
       "1     2    b\n",
       "2     1    c"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── \u001b[01;34manother-bucket\u001b[00m\r\n",
      "├── data-toolz-demo.ipynb\r\n",
      "├── docker-compose.yaml\r\n",
      "├── \u001b[01;34mexample-bucket\u001b[00m\r\n",
      "│   └── data.txt\r\n",
      "└── \u001b[01;34mmy-parquet-file\u001b[00m\r\n",
      "    └── 1649187138447104000-7b62420d-d595-440b-ab7c-702e4cf920f0\r\n",
      "\r\n",
      "3 directories, 4 files\r\n"
     ]
    }
   ],
   "source": [
    "dio.write(df, \"my-parquet-file\")\n",
    "\n",
    "df_read = dio.read(\"my-parquet-file\")\n",
    "df_read\n",
    "\n",
    "# note the created folder/prefix - default settings do not result in idempotent operations!\n",
    "!tree && rm -rf my-parquet-file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-father",
   "metadata": {},
   "source": [
    "---\n",
    "### Advanced writing\n",
    "* `write(..., suffix=...)` - specify a custom output suffix\n",
    "\n",
    "* `write(..., partition_by=...)` - specifies output partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "severe-worship",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col1 col2\n",
       "0     1    a\n",
       "1     2    b\n",
       "2     1    c"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"col1\": [1, 2, 1], \"col2\": [\"a\", \"b\", \"c\"]})\n",
    "df\n",
    "\n",
    "from datatoolz.io import DataIO\n",
    "dio = DataIO()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-pressure",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### With `suffix=\"\"` the dataframe will be written under `path` as a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "exciting-lancaster",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col1 col2\n",
       "0     1    a\n",
       "1     2    b\n",
       "2     1    c"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── \u001b[01;34manother-bucket\u001b[00m\r\n",
      "├── data-toolz-demo.ipynb\r\n",
      "├── docker-compose.yaml\r\n",
      "├── \u001b[01;34mexample-bucket\u001b[00m\r\n",
      "│   └── data.txt\r\n",
      "└── my-parquet-file\r\n",
      "\r\n",
      "2 directories, 4 files\r\n"
     ]
    }
   ],
   "source": [
    "dio.write(df, path=\"my-parquet-file\", suffix=\"\")\n",
    "dio.read(\"my-parquet-file\")\n",
    "\n",
    "!tree && rm -rf my-parquet-file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-visitor",
   "metadata": {},
   "source": [
    "---\n",
    "#### With `suffix=\"string\"` the dataframe will be written under `path/suffix` as a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "peripheral-trader",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col1 col2\n",
       "0     1    a\n",
       "1     2    b\n",
       "2     1    c"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── \u001b[01;34manother-bucket\u001b[00m\r\n",
      "├── data-toolz-demo.ipynb\r\n",
      "├── docker-compose.yaml\r\n",
      "├── \u001b[01;34mexample-bucket\u001b[00m\r\n",
      "│   └── data.txt\r\n",
      "└── \u001b[01;34mmy-parquet-file\u001b[00m\r\n",
      "    └── my-file\r\n",
      "\r\n",
      "3 directories, 4 files\r\n"
     ]
    }
   ],
   "source": [
    "dio.write(df, path=\"my-parquet-file\", suffix=\"my-file\")\n",
    "dio.read(\"my-parquet-file\")\n",
    "\n",
    "!tree && rm -rf my-parquet-file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-remainder",
   "metadata": {},
   "source": [
    "---\n",
    "#### With `suffix=[\"string1\", \"string2\", ...]` the dataframe will be written in `path` under multiple files listed in `suffix` (uniform split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "stunning-diving",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col1 col2\n",
       "0     1    a\n",
       "1     2    b\n",
       "2     1    c"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col1 col2\n",
       "0     1    a\n",
       "1     2    b"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── \u001b[01;34manother-bucket\u001b[00m\r\n",
      "├── data-toolz-demo.ipynb\r\n",
      "├── docker-compose.yaml\r\n",
      "├── \u001b[01;34mexample-bucket\u001b[00m\r\n",
      "│   └── data.txt\r\n",
      "└── \u001b[01;34mmy-parquet-file\u001b[00m\r\n",
      "    ├── my-file-1\r\n",
      "    └── my-file-2\r\n",
      "\r\n",
      "3 directories, 5 files\r\n"
     ]
    }
   ],
   "source": [
    "dio.write(df, path=\"my-parquet-file\", suffix=[\"my-file-1\", \"my-file-2\"])\n",
    "dio.read(\"my-parquet-file\")\n",
    "\n",
    "# read only one file\n",
    "dio.read(\"my-parquet-file/my-file-1\")\n",
    "\n",
    "!tree && rm -rf my-parquet-file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-pursuit",
   "metadata": {},
   "source": [
    "---\n",
    "#### With `partition_by=[\"field\"]` the output in `path` will be additionally split by partition value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "mounted-window",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col1 col2\n",
       "0     1    a\n",
       "1     1    c\n",
       "2     2    b"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col1 col2\n",
       "0     1    a\n",
       "1     1    c"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── \u001b[01;34manother-bucket\u001b[00m\r\n",
      "├── data-toolz-demo.ipynb\r\n",
      "├── docker-compose.yaml\r\n",
      "├── \u001b[01;34mexample-bucket\u001b[00m\r\n",
      "│   └── data.txt\r\n",
      "└── \u001b[01;34mmy-parquet-file\u001b[00m\r\n",
      "    ├── \u001b[01;34mcol1=1\u001b[00m\r\n",
      "    │   └── 1649187139107421000-f28157d2-c1c0-4451-8777-37cccfca482e\r\n",
      "    └── \u001b[01;34mcol1=2\u001b[00m\r\n",
      "        └── 1649187139108683000-b5adb86a-91b8-4d72-a26f-9f7c2b40177d\r\n",
      "\r\n",
      "5 directories, 5 files\r\n"
     ]
    }
   ],
   "source": [
    "dio.write(df, path=\"my-parquet-file\", partition_by=[\"col1\"])\n",
    "dio.read(\"my-parquet-file\")\n",
    "\n",
    "# read a single partition\n",
    "dio.read(\"my-parquet-file/col1=1\")\n",
    "\n",
    "!tree && rm -rf my-parquet-file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-indonesia",
   "metadata": {},
   "source": [
    "---\n",
    "#### The output can be partitioned by multiple fields `partition_by=[\"field1\", \"field2\", ...]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "pacific-sheep",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col1 col2\n",
       "0     1    a\n",
       "1     1    c\n",
       "2     2    b"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── \u001b[01;34manother-bucket\u001b[00m\r\n",
      "├── data-toolz-demo.ipynb\r\n",
      "├── docker-compose.yaml\r\n",
      "├── \u001b[01;34mexample-bucket\u001b[00m\r\n",
      "│   └── data.txt\r\n",
      "└── \u001b[01;34mmy-parquet-file\u001b[00m\r\n",
      "    ├── \u001b[01;34mcol1=1\u001b[00m\r\n",
      "    │   ├── \u001b[01;34mcol2=a\u001b[00m\r\n",
      "    │   │   └── 1649187139279990000-9bf19b58-f19f-4285-935d-17452d127ba1\r\n",
      "    │   └── \u001b[01;34mcol2=c\u001b[00m\r\n",
      "    │       └── 1649187139281053000-72a94260-9bf6-4823-ad90-592ee927c97d\r\n",
      "    └── \u001b[01;34mcol1=2\u001b[00m\r\n",
      "        └── \u001b[01;34mcol2=b\u001b[00m\r\n",
      "            └── 1649187139281873000-9658b50b-693c-440f-9a18-9fa2b4cc3c94\r\n",
      "\r\n",
      "8 directories, 6 files\r\n"
     ]
    }
   ],
   "source": [
    "dio.write(df, path=\"my-parquet-file\", partition_by=[\"col1\", \"col2\"])\n",
    "dio.read(\"my-parquet-file\")\n",
    "\n",
    "!tree && rm -rf my-parquet-file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-pitch",
   "metadata": {},
   "source": [
    "---\n",
    "#### You can overwrite the default partition_transformer resulting in different output path building e.g.\n",
    "* default: `my-parquet-file/col1=1/1649112618853172000-25105f49-24dd-443b-b6bf-a5ca8f7a18d9`\n",
    "* custom: `my-parquet-file/1/fixed-name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "straight-tomorrow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col1 col2\n",
       "0     1    a\n",
       "1     1    c\n",
       "2     2    b"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── \u001b[01;34manother-bucket\u001b[00m\r\n",
      "├── data-toolz-demo.ipynb\r\n",
      "├── docker-compose.yaml\r\n",
      "├── \u001b[01;34mexample-bucket\u001b[00m\r\n",
      "│   └── data.txt\r\n",
      "└── \u001b[01;34mmy-parquet-file\u001b[00m\r\n",
      "    ├── \u001b[01;34m1\u001b[00m\r\n",
      "    │   └── fixed-name\r\n",
      "    └── \u001b[01;34m2\u001b[00m\r\n",
      "        └── fixed-name\r\n",
      "\r\n",
      "5 directories, 5 files\r\n"
     ]
    }
   ],
   "source": [
    "def custom_partition_transformer(prefix, partitions, values, suffix):\n",
    "    partition_part = \"/\".join(map(str, values))\n",
    "    return f\"{prefix}/{partition_part}/fixed-name\"\n",
    "\n",
    "dio = DataIO(partition_transformer=custom_partition_transformer)\n",
    "\n",
    "dio.write(df, path=\"my-parquet-file\", partition_by=[\"col1\"])\n",
    "dio.read(\"my-parquet-file\")\n",
    "\n",
    "!tree && rm -rf my-parquet-file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earlier-poetry",
   "metadata": {},
   "source": [
    "---\n",
    "#### Additionally the partition columns can be dropped from output to reduce redundancy\n",
    "\n",
    "__NOTE__: this process is NOT REVERSABLE by default!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "temporal-subdivision",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  col2\n",
       "0    a\n",
       "1    c"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── \u001b[01;34manother-bucket\u001b[00m\r\n",
      "├── data-toolz-demo.ipynb\r\n",
      "├── docker-compose.yaml\r\n",
      "├── \u001b[01;34mexample-bucket\u001b[00m\r\n",
      "│   └── data.txt\r\n",
      "└── \u001b[01;34mmy-parquet-file\u001b[00m\r\n",
      "    ├── \u001b[01;34mcol1=1\u001b[00m\r\n",
      "    │   └── 1649187139609740000-4be580c0-82c4-41bb-87ea-4e6f30cb99f2\r\n",
      "    └── \u001b[01;34mcol1=2\u001b[00m\r\n",
      "        └── 1649187139611098000-abfd8ceb-650e-45e9-8848-55fe1a7a306b\r\n",
      "\r\n",
      "5 directories, 5 files\r\n"
     ]
    }
   ],
   "source": [
    "dio = DataIO()\n",
    "\n",
    "dio.write(df, path=\"my-parquet-file\", partition_by=[\"col1\"], drop_partitions=True)\n",
    "dio.read(\"my-parquet-file/col1=1\")\n",
    "\n",
    "!tree && rm -rf my-parquet-file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-somalia",
   "metadata": {},
   "source": [
    "---\n",
    "### Other file types\n",
    "\n",
    "Basides `parquet` the following types are handled\n",
    "\n",
    "* `write(..., filetype=\"jsonlines\")`\n",
    "\n",
    "* `write(..., filetype=\"dsv\")`\n",
    "\n",
    "* `write(..., filetype=\"dsv\", **pandas_kwargs)`\n",
    "\n",
    "\n",
    "Both types support compression via `gzip=True`.\n",
    "\n",
    "`pandas_kwargs` is passed to `pandas.DataFrame.to_csv` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "smaller-plasma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col1 col2\n",
       "0     1    a\n",
       "1     2    b\n",
       "2     1    c"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"col1\": [1, 2, 1], \"col2\": [\"a\", \"b\", \"c\"]})\n",
    "df\n",
    "\n",
    "from datatoolz.io import DataIO\n",
    "dio = DataIO()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-cocktail",
   "metadata": {},
   "source": [
    "---\n",
    "#### Write gzip-ed jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "deluxe-planning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col1 col2\n",
       "0     1    a\n",
       "1     2    b\n",
       "2     1    c"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── \u001b[01;34manother-bucket\u001b[00m\r\n",
      "├── data-toolz-demo.ipynb\r\n",
      "├── docker-compose.yaml\r\n",
      "├── \u001b[01;34mexample-bucket\u001b[00m\r\n",
      "│   └── data.txt\r\n",
      "└── \u001b[01;31mmy-data.json.gz\u001b[00m\r\n",
      "\r\n",
      "2 directories, 4 files\r\n",
      "{\"col1\":1,\"col2\":\"a\"}\r\n",
      "{\"col1\":2,\"col2\":\"b\"}\r\n",
      "{\"col1\":1,\"col2\":\"c\"}\r\n"
     ]
    }
   ],
   "source": [
    "dio.write(df, path=\"my-data.json.gz\", filetype=\"jsonlines\", gzip=True, suffix=\"\")\n",
    "dio.read(\"my-data.json.gz\", filetype=\"jsonlines\", gzip=True)\n",
    "\n",
    "!tree && gunzip my-data.json.gz && cat my-data.json && rm my-data.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-barbados",
   "metadata": {},
   "source": [
    "---\n",
    "#### Write tab-separated (default) file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "recreational-perth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  col1 col2\n",
       "0    1    a\n",
       "1    2    b\n",
       "2    1    c"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── \u001b[01;34manother-bucket\u001b[00m\r\n",
      "├── data-toolz-demo.ipynb\r\n",
      "├── docker-compose.yaml\r\n",
      "├── \u001b[01;34mexample-bucket\u001b[00m\r\n",
      "│   └── data.txt\r\n",
      "└── my-data.txt\r\n",
      "\r\n",
      "2 directories, 4 files\r\n",
      "col1\tcol2\r\n",
      "1\ta\r\n",
      "2\tb\r\n",
      "1\tc\r\n"
     ]
    }
   ],
   "source": [
    "dio.write(df, path=\"my-data.txt\", filetype=\"dsv\", suffix=\"\")\n",
    "dio.read(\"my-data.txt\", filetype=\"dsv\")\n",
    "\n",
    "!tree && cat my-data.txt && rm my-data.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-silicon",
   "metadata": {},
   "source": [
    "---\n",
    "#### Write |-separated file without header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bridal-alexander",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1\n",
       "0  1  a\n",
       "1  2  b\n",
       "2  1  c"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── \u001b[01;34manother-bucket\u001b[00m\r\n",
      "├── data-toolz-demo.ipynb\r\n",
      "├── docker-compose.yaml\r\n",
      "├── \u001b[01;34mexample-bucket\u001b[00m\r\n",
      "│   └── data.txt\r\n",
      "└── my-data.txt\r\n",
      "\r\n",
      "2 directories, 4 files\r\n",
      "1|a\r\n",
      "2|b\r\n",
      "1|c\r\n"
     ]
    }
   ],
   "source": [
    "dio.write(df, path=\"my-data.txt\", filetype=\"dsv\", sep=\"|\", header=None, suffix=\"\")\n",
    "dio.read(\"my-data.txt\", filetype=\"dsv\", sep=\"|\", header=None)\n",
    "\n",
    "!tree && cat my-data.txt && rm my-data.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-physics",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "You have a large dataset which doesn't fit into memory.\n",
    "The dataset was stored in multiple small partitions.\n",
    "\n",
    "Your goal is to aggregate it (compute `max` of `field`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "acute-registrar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── \u001b[01;34manother-bucket\u001b[00m\r\n",
      "├── \u001b[01;34mbig-data\u001b[00m\r\n",
      "│   ├── file_0\r\n",
      "│   ├── file_1\r\n",
      "│   ├── file_2\r\n",
      "│   ├── file_3\r\n",
      "│   ├── file_4\r\n",
      "│   ├── file_5\r\n",
      "│   ├── file_6\r\n",
      "│   ├── file_7\r\n",
      "│   ├── file_8\r\n",
      "│   └── file_9\r\n",
      "├── data-toolz-demo.ipynb\r\n",
      "├── docker-compose.yaml\r\n",
      "└── \u001b[01;34mexample-bucket\u001b[00m\r\n",
      "    └── data.txt\r\n",
      "\r\n",
      "3 directories, 13 files\r\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\"field\": np.random.random(1000)})\n",
    "\n",
    "dio = DataIO()\n",
    "dio.write(df, \"big-data\", suffix=(f\"file_{i}\" for i in range(10)))\n",
    "\n",
    "!tree\n",
    "\n",
    "# write code here\n",
    "\n",
    "\n",
    "!rm -rf big-data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-miracle",
   "metadata": {},
   "source": [
    "---\n",
    "## JsonLogger (datatoolz.logging.JsonLogger)\n",
    "\n",
    "This is a simple structured-logging class. You can use it to publish application logs as JSON encoded strings.\n",
    "\n",
    "Initialisation\n",
    "```python\n",
    "from datatoolz.logging import JsonLogger\n",
    "\n",
    "logger = JsonLogger(name=\"my-app\", env=\"dev\")\n",
    "```\n",
    "\n",
    "__Disclaimer__: currently this module is not further developed as may be removed alltogether in the future, as there are better, more specialized packages with the same functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "occasional-timing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datatoolz.logging import JsonLogger\n",
    "\n",
    "logger = JsonLogger(name=\"my-app\", env=\"dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-dollar",
   "metadata": {},
   "source": [
    "---\n",
    "#### Log a simple message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "demographic-shaft",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"logger\": {\"application\": \"my-app\", \"environment\": \"dev\"}, \"level\": \"info\", \"timestamp\": \"2022-04-05 19:32:20.497457\", \"message\": \"Wubba lubba dub dub\"}\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Wubba lubba dub dub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-sitting",
   "metadata": {},
   "source": [
    "---\n",
    "#### Log `extra` arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "hidden-cooler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"logger\": {\"application\": \"my-app\", \"environment\": \"dev\"}, \"level\": \"info\", \"timestamp\": \"2022-04-05 19:32:20.504213\", \"message\": \"Wubba lubba dub dub\", \"extra\": {\"purpose\": \"pass butter\"}}\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Wubba lubba dub dub\", purpose=\"pass butter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "heavy-delay",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"logger\": {\"application\": \"my-app\", \"environment\": \"dev\"}, \"level\": \"info\", \"timestamp\": \"2022-04-05 19:32:20.510440\", \"message\": \"Wubba lubba dub dub\", \"extra\": {\"object\": {\"name\": \"Jerry\"}}}\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Wubba lubba dub dub\", object={\"name\": \"Jerry\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subjective-sound",
   "metadata": {},
   "source": [
    "---\n",
    "#### Use as decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "elder-geneva",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"logger\": {\"application\": \"my-app\", \"environment\": \"dev\"}, \"level\": \"info\", \"timestamp\": \"2022-04-05 19:32:20.516421\", \"message\": \"Computing the sum\", \"extra\": {\"function\": \"my_sum\", \"memory\": {\"current\": 0, \"peak\": 0}, \"duration\": 4.4800000011946395e-06}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@logger.decorate(\"Computing the sum\")\n",
    "def my_sum(x, y):\n",
    "    return x + y\n",
    "\n",
    "my_sum(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-robertson",
   "metadata": {},
   "source": [
    "---\n",
    "#### Provide callback `extra` attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "simplified-document",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"logger\": {\"application\": \"my-app\", \"environment\": \"dev\"}, \"level\": \"info\", \"timestamp\": \"2022-04-05 19:32:20.529520\", \"message\": \"Computing the sum\", \"extra\": {\"function\": \"my_sum\", \"memory\": {\"current\": 29814, \"peak\": 86938}, \"duration\": 9.930000004487738e-07, \"python\": \"/Library/Frameworks/Python.framework/Versions/3.8/bin/python3.8\", \"is_even\": true, \"is_negative\": true}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-4"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@logger.decorate(\n",
    "    \"Computing the sum\", \n",
    "    python=sys.executable, \n",
    "    is_even=lambda x: x % 2 == 0, \n",
    "    is_negative=lambda x: x < 0\n",
    ")\n",
    "def my_sum(x, y):\n",
    "    return x + y\n",
    "\n",
    "my_sum(3, -7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-struggle",
   "metadata": {},
   "source": [
    "---\n",
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-trigger",
   "metadata": {},
   "source": [
    "1. `s3` (or any other cloud storage) is not a file system!\n",
    "  * a file system wrapper is conveniant but could result in sub-optimal implementatio\n",
    "\n",
    "\n",
    "2. There are no \"pre-checks\" - the package will try to execute any requested command e.g. \"list all objects in a bucket\"\n",
    "\n",
    "\n",
    "3. Copy between buckets - possible but not always optimal (see point 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-museum",
   "metadata": {},
   "source": [
    "---\n",
    "### If you like these features and would like to see more or if you find a bug\n",
    "* __contributors are welcome__\n",
    "* drop by https://github.com/grzegorzme/data-toolz and create a new issue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-toolz-demo",
   "language": "python",
   "name": "data-toolz-demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
